{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/tgs-salt-identification-challenge/discussion/67762#399663\n",
    "class scSELayer(nn.Module):\n",
    "    def __init__(self, n_in, r = 16):\n",
    "        assert n_in % r == 0, f'in channel count needs to be divisible by r == {r}'\n",
    "        super().__init__()\n",
    "        self.cSE = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(n_in,n_in//r,1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(n_in//r,n_in,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(n_in,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.cSE(x) + x * self.sSE(x)\n",
    "    \n",
    "class sSELayer(nn.Module):\n",
    "    def __init__(self, n_in):\n",
    "        super().__init__()\n",
    "        self.sSE = nn.Sequential(\n",
    "            nn.Conv2d(n_in,1,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sSE(x)\n",
    "    \n",
    "class HCBlock(nn.Module):\n",
    "    '''Hypercolumn block - reduces num of channels and interpolates'''\n",
    "    def __init__(self, n_in, out_sz=256):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_in, 16, 1)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.out_sz = out_sz\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.bn(x)\n",
    "        return interpolate(x, (self.out_sz, self.out_sz), mode='bilinear', align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out, kernel_size=2, output_padding=0, padding=0, stride=2):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, kernel_size, stride=stride, output_padding=output_padding, padding=padding)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "        self.out_channels = n_out\n",
    "        \n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res34UnetBlock(UnetBlock):\n",
    "    def __init__(self, up_in, x_in, n_out, k):\n",
    "        super().__init__(up_in, x_in, n_out, kernel_size=k, padding=(k-2)//2)\n",
    "    \n",
    "class BasicRes34Unet(nn.Module):        \n",
    "    def __init__(self, mult=1, k=2):\n",
    "        super().__init__()\n",
    "        base = nn.Sequential(*list(pretrainedmodels.resnet34().children())[:-2])\n",
    "        self.down1 = base[:3]\n",
    "        self.down2 = base[3:5]\n",
    "        self.down3 = base[5:6]\n",
    "        self.down4 = base[6:7]\n",
    "        self.down5 = base[7:]\n",
    "        \n",
    "        self.up1 = Res34UnetBlock(512,256,192 * mult,k)\n",
    "        self.up2 = Res34UnetBlock(192 * mult,128,96 * mult,k)\n",
    "        self.up3 = Res34UnetBlock(96 * mult,64,32 * mult,k)\n",
    "        self.up4 = Res34UnetBlock(32 * mult,64,32 * mult,k)\n",
    "        self.up5 = Res34UnetBlock(32 * mult,3,16 * mult,k)\n",
    "        \n",
    "        self.up6 = nn.ConvTranspose2d(16 * mult, 1, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.up1(mid, d4)\n",
    "        x = self.up2(x, d3)\n",
    "        x = self.up3(x, d2)\n",
    "        x = self.up4(x, d1)\n",
    "        x = self.up5(x, inp)\n",
    "        x = self.up6(x)\n",
    "  \n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class Res34UnetSE(BasicRes34Unet):\n",
    "    def __init__(self, mult=1, k=2):\n",
    "        super().__init__(mult, k)\n",
    "        \n",
    "        self.se1 = scSELayer(self.up1.out_channels)\n",
    "        self.se2 = scSELayer(self.up2.out_channels)\n",
    "        self.se3 = scSELayer(self.up3.out_channels)\n",
    "        self.se4 = scSELayer(self.up4.out_channels)\n",
    "        self.se5 = scSELayer(self.up5.out_channels)\n",
    "        \n",
    "        self.se_feat1 = scSELayer(64)\n",
    "        self.se_feat2 = scSELayer(64)\n",
    "        self.se_feat3 = scSELayer(128)\n",
    "        self.se_feat4 = scSELayer(256)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.se_feat1(self.down1(x))\n",
    "        d2 = self.se_feat2(self.down2(d1))\n",
    "        d3 = self.se_feat3(self.down3(d2))\n",
    "        d4 = self.se_feat4(self.down4(d3))\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.se1(self.up1(mid, d4))\n",
    "        x = self.se2(self.up2(x, d3))\n",
    "        x = self.se3(self.up3(x, d2))\n",
    "        x = self.se4(self.up4(x, d1))\n",
    "        x = self.se5(self.up5(x, inp))\n",
    "        x = self.up6(x)\n",
    "        \n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]\n",
    "\n",
    "class Res34UnetSE_HC(Res34UnetSE):\n",
    "    def __init__(self, mult=1, k=2, sz=128, comb_hc_channels=8):\n",
    "        super().__init__(mult, k)\n",
    "        \n",
    "        self.hc1 = HCBlock(self.up1.out_channels, out_sz=sz)\n",
    "        self.hc2 = HCBlock(self.up2.out_channels, out_sz=sz)\n",
    "        self.hc3 = HCBlock(self.up3.out_channels, out_sz=sz)\n",
    "        self.hc4 = HCBlock(self.up4.out_channels, out_sz=sz)\n",
    "        \n",
    "        self.hc_comb = nn.Conv2d(64, comb_hc_channels, 3, padding=1)\n",
    "        self.hc_bn = nn.BatchNorm2d(comb_hc_channels)\n",
    "        \n",
    "        self.up6 = nn.ConvTranspose2d(16 * mult + comb_hc_channels, 1, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        d1 = self.se_feat1(d1)\n",
    "        d2 = self.se_feat2(d2)\n",
    "        d3 = self.se_feat3(d3)\n",
    "        d4 = self.se_feat4(d4)\n",
    "        \n",
    "        x = self.se1(self.up1(mid, d4))\n",
    "        hc1 = self.hc1(x)\n",
    "        \n",
    "        x = self.se2(self.up2(x, d3))\n",
    "        hc2 = self.hc2(x)\n",
    "        \n",
    "        x = self.se3(self.up3(x, d2))\n",
    "        hc3 = self.hc3(x)\n",
    "        \n",
    "        x = self.se4(self.up4(x, d1))\n",
    "        hc4 = self.hc4(x)\n",
    "        \n",
    "        x = self.se5(self.up5(x, inp))\n",
    "        \n",
    "        hc = self.hc_comb(torch.cat((hc1, hc2, hc3, hc4), dim=1))\n",
    "        hc = self.hc_bn(F.relu(hc))\n",
    "        \n",
    "        x = torch.cat((x, hc), dim=1)\n",
    "        x = self.up6(x)\n",
    "        \n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicUnetSEresnext50(nn.Module):        \n",
    "    def __init__(self, mult=1, k=2):\n",
    "        super().__init__()\n",
    "        base = nn.Sequential(*list(pretrainedmodels.se_resnext50_32x4d().children())[:-2])\n",
    "\n",
    "        self.down1 = base[0][:3] \n",
    "        self.down2 = nn.Sequential(base[0][3:], base[1:2])\n",
    "        self.down3 = base[2:3]\n",
    "        self.down4 = base[3:4]\n",
    "        self.down5 = base[4:]\n",
    "        \n",
    "        self.up1 = Res34UnetBlock(2048,1024,768,k)\n",
    "        self.up2 = Res34UnetBlock(768,512,320,k)\n",
    "        self.up3 = Res34UnetBlock(320,256,160,k)\n",
    "        self.up4 = Res34UnetBlock(160,64,80,k)\n",
    "        self.up5 = Res34UnetBlock(80,3,32,k)\n",
    "        \n",
    "        self.up6 = nn.ConvTranspose2d(32, 1, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.up1(mid, d4)\n",
    "        x = self.up2(x, d3)\n",
    "        x = self.up3(x, d2)\n",
    "        x = self.up4(x, d1)\n",
    "        x = self.up5(x, inp)\n",
    "        x = self.up6(x)\n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]\n",
    "    \n",
    "class UnetSEresnext50SE(BasicUnetSEresnext50):\n",
    "    def __init__(self, mult=1, k=2):\n",
    "        super().__init__(mult, k)\n",
    "        \n",
    "        self.se_down1 = sSELayer(64)\n",
    "        self.se_down2 = sSELayer(256)\n",
    "        self.se_down3 = sSELayer(512)\n",
    "        self.se_down4 = sSELayer(1024)\n",
    "\n",
    "        self.se_up1 = scSELayer(self.up1.out_channels)\n",
    "        self.se_up2 = scSELayer(self.up2.out_channels)\n",
    "        self.se_up3 = scSELayer(self.up3.out_channels)\n",
    "        self.se_up4 = scSELayer(self.up4.out_channels)\n",
    "        self.se_up5 = scSELayer(self.up5.out_channels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.se_down1(self.down1(x))\n",
    "        d2 = self.se_down2(self.down2(d1))\n",
    "        d3 = self.se_down3(self.down3(d2))\n",
    "        d4 = self.se_down4(self.down4(d3))\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.se_up1(self.up1(mid, d4))\n",
    "        x = self.se_up2(self.up2(x, d3))\n",
    "        x = self.se_up3(self.up3(x, d2))\n",
    "        x = self.se_up4(self.up4(x, d1))\n",
    "        x = self.se_up5(self.up5(x, inp))\n",
    "        x = self.up6(x)\n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]\n",
    "    \n",
    "class UnetSEresnext50SE_HC(UnetSEresnext50SE):\n",
    "    def __init__(self, mult=1, k=2, sz=128, comb_hc_channels=16):\n",
    "        super().__init__(mult, k)\n",
    "                           \n",
    "        self.hc1 = HCBlock(self.up1.out_channels, out_sz=sz)\n",
    "        self.hc2 = HCBlock(self.up2.out_channels, out_sz=sz)\n",
    "        self.hc3 = HCBlock(self.up3.out_channels, out_sz=sz)\n",
    "        self.hc4 = HCBlock(self.up4.out_channels, out_sz=sz)\n",
    "        \n",
    "        self.hc_comb = nn.Conv2d(64, comb_hc_channels, 1)\n",
    "        self.hc_bn = nn.BatchNorm2d(comb_hc_channels)\n",
    "\n",
    "        self.up6 = nn.ConvTranspose2d(32+comb_hc_channels, 1, 1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.se_down1(self.down1(x))\n",
    "        d2 = self.se_down2(self.down2(d1))\n",
    "        d3 = self.se_down3(self.down3(d2))\n",
    "        d4 = self.se_down4(self.down4(d3))\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.se_up1(self.up1(mid, d4))\n",
    "        hc1 = self.hc1(x)\n",
    "        x = self.se_up2(self.up2(x, d3))\n",
    "        hc2 = self.hc2(x)\n",
    "        x = self.se_up3(self.up3(x, d2))\n",
    "        hc3 = self.hc3(x)\n",
    "        x = self.se_up4(self.up4(x, d1))\n",
    "        hc4 = self.hc4(x)\n",
    "        \n",
    "        hc = self.hc_comb(torch.cat((hc1, hc2, hc3, hc4), dim=1))\n",
    "        hc = self.hc_bn(F.relu(hc))\n",
    "                           \n",
    "        x = self.se_up5(self.up5(x, inp))\n",
    "        x = torch.cat((x, hc), dim=1)\n",
    "        x = self.up6(x)\n",
    "        assert (x.shape[1] == 1)\n",
    "        return x[:, 0]\n",
    "    \n",
    "# this model has a couple of likely issues:\n",
    "#   - the classifier part  should probably be way more complex\n",
    "#   - it would probably be better to return apply loss to each of the hcXs instead of to hs_comb\n",
    "#   - the losses (not visible here) that I implemented should probably only apply segmentation loss\n",
    "#     to x and hot hcXs where mask is all zeros      \n",
    "class UnetSEresnext50SE_HC_deep_sup(UnetSEresnext50SE_HC):\n",
    "    def __init__(self, mult=1, k=2, sz=128, comb_hc_channels=16):\n",
    "        super().__init__(mult, k, sz=sz, comb_hc_channels=comb_hc_channels)\n",
    "        \n",
    "        self.classifier = nn.Linear(2048, 1)\n",
    "        self.hc_class = nn.Conv2d(comb_hc_channels, 1, 1)\n",
    "                           \n",
    "    def forward(self,x):\n",
    "        inp = x\n",
    "        d1 = self.se_down1(self.down1(x))\n",
    "        d2 = self.se_down2(self.down2(d1))\n",
    "        d3 = self.se_down3(self.down3(d2))\n",
    "        d4 = self.se_down4(self.down4(d3))\n",
    "        \n",
    "        mid = F.relu(self.down5(d4))\n",
    "        \n",
    "        x = self.se_up1(self.up1(mid, d4))\n",
    "        hc1 = self.hc1(x)\n",
    "        x = self.se_up2(self.up2(x, d3))\n",
    "        hc2 = self.hc2(x)\n",
    "        x = self.se_up3(self.up3(x, d2))\n",
    "        hc3 = self.hc3(x)\n",
    "        x = self.se_up4(self.up4(x, d1))\n",
    "        hc4 = self.hc4(x)\n",
    "        \n",
    "        hc = self.hc_comb(torch.cat((hc1, hc2, hc3, hc4), dim=1))\n",
    "        hc = self.hc_bn(F.relu(hc))\n",
    "                           \n",
    "        x = self.se_up5(self.up5(x, inp))\n",
    "        x = torch.cat((x, hc), dim=1)\n",
    "        x = self.up6(x)\n",
    "        \n",
    "        ### classification\n",
    "        avg_pool = mid.view((mid.shape[0], mid.shape[1], -1)).mean(2)\n",
    "        cl = self.classifier(avg_pool)\n",
    "        \n",
    "        ### hc segmentation\n",
    "        hc_out = self.hc_class(hc)\n",
    "        \n",
    "        assert (x.shape[1] == 1)\n",
    "        assert (hc_out.shape[1] == 1)\n",
    "        return x[:, 0], hc_out[:, 0], cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = get_data_bunch()\n",
    "# learn = get_learner(db)\n",
    "# ims = torch.zeros(4,3,128,128).cuda()\n",
    "# learn.model(ims).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved callbacks\n",
    "class SaveBest(Callback):\n",
    "    def __init__(self):\n",
    "        self.iou = 0\n",
    "    def on_epoch_end(self, epoch, num_batch, smooth_loss, last_metrics, **kwargs): \n",
    "        iou = last_metrics[-1]\n",
    "        if iou > self.iou:\n",
    "            self.iou = iou\n",
    "            learn.save(f'{name}_best_iou_fold{fold}')\n",
    "            \n",
    "class ReduceLROnPlateau(Callback):\n",
    "    def __init__(self, learn, patience=5, div_factor=10, grace=0, delta=1e-4):\n",
    "        self.learn = learn\n",
    "        self.patience = patience\n",
    "        self.div_factor = div_factor\n",
    "        \n",
    "        self.iou = 0\n",
    "        self.epochs_without_improv = 0\n",
    "        self.delta = delta\n",
    "        self.grace = grace # number of epochs to remain inactive after train start\n",
    "                           # useful for retraining starting with higher lr\n",
    "    def on_epoch_end(self, epoch, num_batch, smooth_loss, last_metrics, **kwargs): \n",
    "        if self.grace > 0:\n",
    "            self.grace -= 1\n",
    "            return\n",
    "        \n",
    "        iou = last_metrics[-1]\n",
    "        if iou - self.iou > self.delta:\n",
    "            self.epochs_without_improv = 0\n",
    "            self.iou = iou\n",
    "        else:\n",
    "            self.epochs_without_improv += 1\n",
    "        if self.epochs_without_improv == self.patience:\n",
    "            lr = self.learn.opt.read_val('lr')\n",
    "            self.learn.opt.lr = np.array(lr) / self.div_factor\n",
    "            print(f'Reducing lr to: {self.learn.opt.lr}')\n",
    "            self.epochs_without_improv = 0\n",
    "            \n",
    "    \n",
    "class StopTrain(Callback):\n",
    "    def __init__(self, learn, patience=5, delta=1e-4):\n",
    "        self.learn = learn\n",
    "        self.patience = patience\n",
    "        \n",
    "        self.iou = 0\n",
    "        self.delta = delta\n",
    "        self.epochs_without_improv = 0\n",
    "    def on_epoch_end(self, epoch, num_batch, smooth_loss, last_metrics, **kwargs): \n",
    "        iou = last_metrics[-1]\n",
    "        if iou - self.iou > self.delta:\n",
    "            self.epochs_without_improv = 0\n",
    "            self.iou = iou\n",
    "        else:\n",
    "            self.epochs_without_improv += 1\n",
    "        if self.epochs_without_improv == self.patience:\n",
    "            lr = self.learn.opt.read_val('lr')\n",
    "            print(f'Finishing training with lr: {lr}')\n",
    "            return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
